{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMPLEMENTING A NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets  \n",
    "import numpy as np\n",
    "import math \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this func for computing perdictions\n",
    "def prediction(model , x ) :\n",
    "    w1 , w2 , b1 , b2 = model['w1'] , model['w2'] , model['b1'] , model['b2'] \n",
    "    z1 = x.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    res = softmax(z2)\n",
    "    \n",
    "    return np.argmax(res, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this func for computing loss \n",
    "def compute_loss(model , x , y , reg_lambda):\n",
    "    #get length of the data\n",
    "    num_examples = len(y)\n",
    "    \n",
    "    #get the predictions by using this func\n",
    "    w1 , w2 , b1 , b2 = model['w1'] , model['w2'] , model['b1'] , model['b2'] \n",
    "    z1 = x.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    prob = softmax(z2)\n",
    "    \n",
    "    #get the loss of data \n",
    "    logprop = -np.log(prob[range(num_examples), y])\n",
    "    data_loss = np.sum(logprop)\n",
    "    #add the regulatization term to loss\n",
    "    data_loss += reg_lambda /2  * (np.sum(np.square(w1)) + np.sum(np.square(w2)))\n",
    "    \n",
    "    return 1.0 / num_examples * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def softmax(x):\n",
    "        '''\n",
    "        using softmax for the output layer\n",
    "        '''\n",
    "        x = np.array(x,dtype=float)\n",
    "\n",
    "        #if the array was 1 dim\n",
    "        if len(x.shape) ==1 :\n",
    "            #normalize it \n",
    "            x = x - x.max()\n",
    "            #calculate the softmax\n",
    "            x = np.exp(x) / np.sum(np.exp(x))\n",
    "        else :\n",
    "            m= np.max(x, axis=1)\n",
    "            #make it a coulm vector so every row get normalized by it's max\n",
    "            x -= m.reshape((x.shape[0], 1))\n",
    "            #calculate the softmax\n",
    "            x = np.exp(x) / (np.sum(np.exp(x), axis=1)).reshape(x.shape[0], 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(model , x ,y) :\n",
    "    w1 , w2 , b1 , b2 = model['w1'] , model['w2'] , model['b1'] , model['b2'] \n",
    "    \n",
    "    num_examples = len(x)\n",
    "    \n",
    "    #forward propg    \n",
    "    z1 = x.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "    delta3 = probs\n",
    "    delta3[range(num_examples), y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = delta3.dot(w2.T) * (1 - np.power(a1, 2))\n",
    "    dW1 = np.dot(x.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    #our gradients stored here \n",
    "    dpram={'dw2' : dW2 , 'dw1':dW1 , 'db1' : db1 , 'db2' : db2}\n",
    "    return dpram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(x , y , hid_n , inp_n , out_n , reg  , eps , num_passess = 20000) :\n",
    "    #inialize our arrays \n",
    "    np.random.seed(0)\n",
    "    w1 = np.random.randn(inp_n , hid_n) / np.sqrt(inp_n)\n",
    "    w2 = np.random.randn(hid_n , out_n) / np.sqrt(hid_n)\n",
    "    b1 = np.zeros((1 , hid_n))\n",
    "    b2 = np.zeros((1 , out_n))\n",
    "    \n",
    "    #our parameters stored here \n",
    "    model = {}\n",
    "    param = {}\n",
    "    # iteraion \n",
    "    model = {'w1':w1 , 'w2':w2 , 'b1' :b1 , 'b2' :b2}\n",
    "    for i in range(0 , num_passess) :\n",
    "        param = gradient_descent(model , x , y )\n",
    "        \n",
    "        #add regulariation\n",
    "        dw2 = param['dw2'] + reg * w2\n",
    "        dw1 = param['dw1'] + reg * w1\n",
    "        db1 = param['db1']\n",
    "        db2 = param['db2']\n",
    "        \n",
    "        #update our parameters \n",
    "        w1 -= eps * dw1\n",
    "        w2 -= eps * dw2\n",
    "        b1 -= eps * db1 \n",
    "        b2 -= eps * db2\n",
    "        \n",
    "        #assign our new params \n",
    "        model = {'w1' : w1 , 'w2': w2 , 'b1' :b1 , 'b2' :b2}\n",
    "        loss = compute_loss(model, x, y , reg)\n",
    "        if i % 1000 == 0 :\n",
    "            \n",
    "            print \"Loss after iteration %i: %f\" % (i,loss)\n",
    "        \n",
    "    print \"========================== /n\"    \n",
    "    print \"print W1 \" , model['w1']\n",
    "    print \"print W2 \" , model['w2']\n",
    "    print \"print b1 \" , model['b1']\n",
    "    print \"print b2 \" , model['b2']\n",
    "    print \"print the loss\"  , loss\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.432387\n",
      "Loss after iteration 1000: 0.068947\n",
      "Loss after iteration 2000: 0.068926\n",
      "Loss after iteration 3000: 0.071218\n",
      "Loss after iteration 4000: 0.071253\n",
      "Loss after iteration 5000: 0.071278\n",
      "Loss after iteration 6000: 0.071293\n",
      "Loss after iteration 7000: 0.071303\n",
      "Loss after iteration 8000: 0.071308\n",
      "Loss after iteration 9000: 0.071312\n",
      "Loss after iteration 10000: 0.071314\n",
      "Loss after iteration 11000: 0.071315\n",
      "Loss after iteration 12000: 0.071315\n",
      "Loss after iteration 13000: 0.071316\n",
      "Loss after iteration 14000: 0.071316\n",
      "Loss after iteration 15000: 0.071316\n",
      "Loss after iteration 16000: 0.071316\n",
      "Loss after iteration 17000: 0.071316\n",
      "Loss after iteration 18000: 0.071316\n",
      "Loss after iteration 19000: 0.071316\n",
      "========================== /n\n",
      "print W1  [[ 2.93518326 -4.15545598  3.42795842]\n",
      " [ 2.6274756   0.75135518 -0.75175306]]\n",
      "print W2  [[ 4.40472193 -4.34231862]\n",
      " [ 4.84301638 -4.81900141]\n",
      " [-2.66218456  2.78705795]]\n",
      "print b1  [[-2.33806553  4.95656369  1.28485078]]\n",
      "print b2  [[-0.98275564  0.98275564]]\n",
      "print the loss 0.0707579458641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nlet's train our model \\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = .01\n",
    "reg = .01\n",
    "hid_n = 3\n",
    "inp_n = 2\n",
    "out_n = 2\n",
    "\n",
    "#generate our data\n",
    "np.random.seed(0)\n",
    "x, y = datasets.make_moons(200, noise=0.20)\n",
    "\n",
    "#build our model \n",
    "model = build_model(x ,y , hid_n , inp_n , out_n , reg  , eps , 20000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# change some params , and compare the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss was .070 \n",
      "          \n",
      "Loss after iteration 0: 0.432387\n",
      "Loss after iteration 1000: 0.068947\n",
      "Loss after iteration 2000: 0.068926\n",
      "Loss after iteration 3000: 0.071218\n",
      "Loss after iteration 4000: 0.071253\n",
      "Loss after iteration 5000: 0.071278\n",
      "Loss after iteration 6000: 0.071293\n",
      "Loss after iteration 7000: 0.071303\n",
      "Loss after iteration 8000: 0.071308\n",
      "Loss after iteration 9000: 0.071312\n",
      "Loss after iteration 10000: 0.071314\n",
      "Loss after iteration 11000: 0.071315\n",
      "Loss after iteration 12000: 0.071315\n",
      "Loss after iteration 13000: 0.071316\n",
      "Loss after iteration 14000: 0.071316\n",
      "Loss after iteration 15000: 0.071316\n",
      "Loss after iteration 16000: 0.071316\n",
      "Loss after iteration 17000: 0.071316\n",
      "Loss after iteration 18000: 0.071316\n",
      "Loss after iteration 19000: 0.071316\n",
      "Loss after iteration 20000: 0.071316\n",
      "Loss after iteration 21000: 0.071316\n",
      "Loss after iteration 22000: 0.071316\n",
      "Loss after iteration 23000: 0.071316\n",
      "Loss after iteration 24000: 0.071316\n",
      "Loss after iteration 25000: 0.071316\n",
      "Loss after iteration 26000: 0.071316\n",
      "Loss after iteration 27000: 0.071316\n",
      "Loss after iteration 28000: 0.071316\n",
      "Loss after iteration 29000: 0.071316\n",
      "========================== /n\n",
      "print W1  [[ 2.9351883  -4.15545454  3.42798097]\n",
      " [ 2.62748289  0.75135343 -0.75177314]]\n",
      "print W2  [[ 4.38500238 -4.36204663]\n",
      " [ 4.83546465 -4.82663048]\n",
      " [-2.70163847  2.74757453]]\n",
      "print b1  [[-2.33807535  4.95656586  1.28486506]]\n",
      "print b2  [[-0.98278551  0.98278551]]\n",
      "print the loss 0.0707577390425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# increase the num of passes \n",
    "eps = .01\n",
    "reg = .01\n",
    "hid_n = 3\n",
    "inp_n = 2\n",
    "out_n = 2\n",
    "\n",
    "print \"the loss was .070 \"\n",
    "print \"          \"\n",
    "#build our model \n",
    "model = build_model(x ,y , hid_n , inp_n , out_n , reg  , eps , 30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss was .070 \n",
      "          \n",
      "Loss after iteration 0: 0.466461\n",
      "Loss after iteration 1000: 0.066685\n",
      "Loss after iteration 2000: 0.060113\n",
      "Loss after iteration 3000: 0.057100\n",
      "Loss after iteration 4000: 0.055831\n",
      "Loss after iteration 5000: 0.055171\n",
      "Loss after iteration 6000: 0.054800\n",
      "Loss after iteration 7000: 0.057093\n",
      "Loss after iteration 8000: 0.051177\n",
      "Loss after iteration 9000: 0.050966\n",
      "Loss after iteration 10000: 0.050251\n",
      "Loss after iteration 11000: 0.050184\n",
      "Loss after iteration 12000: 0.050372\n",
      "Loss after iteration 13000: 0.050205\n",
      "Loss after iteration 14000: 0.050127\n",
      "Loss after iteration 15000: 0.050209\n",
      "Loss after iteration 16000: 0.059759\n",
      "Loss after iteration 17000: 0.050977\n",
      "Loss after iteration 18000: 0.050130\n",
      "Loss after iteration 19000: 0.051254\n",
      "Loss after iteration 20000: 0.052247\n",
      "Loss after iteration 21000: 0.050127\n",
      "Loss after iteration 22000: 0.052458\n",
      "Loss after iteration 23000: 0.050997\n",
      "Loss after iteration 24000: 0.050142\n",
      "Loss after iteration 25000: 0.058594\n",
      "Loss after iteration 26000: 0.050332\n",
      "Loss after iteration 27000: 0.050171\n",
      "Loss after iteration 28000: 0.064439\n",
      "Loss after iteration 29000: 0.050150\n",
      "========================== /n\n",
      "print W1  [[ 3.96976666  4.95121454  0.49372145  5.07244084]\n",
      " [ 4.62639206 -2.5008324   6.10379608 -5.69457741]]\n",
      "print W2  [[ 6.40474848 -6.39709786]\n",
      " [-6.66640773  6.70618952]\n",
      " [-4.06925852  4.09122906]\n",
      " [-5.63679417  5.65614692]]\n",
      "print b1  [[-3.1791313  -5.68769084 -1.96527487  3.51209003]]\n",
      "print b2  [[-0.80266709  0.80266709]]\n",
      "print the loss 0.0502269744047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# increase the num of nn in hidden layer\n",
    "eps = .01\n",
    "reg = .01\n",
    "hid_n = 4\n",
    "inp_n = 2\n",
    "out_n = 2\n",
    "\n",
    "#generate our data\n",
    "np.random.seed(0)\n",
    "x, y = datasets.make_moons(200, noise=0.20)\n",
    "print \"the loss was .070 \"\n",
    "print \"          \"\n",
    "#build our model \n",
    "model = build_model(x ,y , hid_n , inp_n , out_n , reg  , eps , 30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the loss decrease \n",
    "let's increase the nn in the hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss was .050 \n",
      "          \n",
      "Loss after iteration 0: 0.613897\n",
      "Loss after iteration 1000: 0.053880\n",
      "Loss after iteration 2000: 0.044053\n",
      "Loss after iteration 3000: 0.041367\n",
      "Loss after iteration 4000: 0.040268\n",
      "Loss after iteration 5000: 0.039732\n",
      "Loss after iteration 6000: 0.039444\n",
      "Loss after iteration 7000: 0.039280\n",
      "Loss after iteration 8000: 0.039185\n",
      "Loss after iteration 9000: 0.039128\n",
      "Loss after iteration 10000: 0.039093\n",
      "Loss after iteration 11000: 0.039072\n",
      "Loss after iteration 12000: 0.039059\n",
      "Loss after iteration 13000: 0.039050\n",
      "Loss after iteration 14000: 0.039045\n",
      "Loss after iteration 15000: 0.039042\n",
      "Loss after iteration 16000: 0.039040\n",
      "Loss after iteration 17000: 0.039038\n",
      "Loss after iteration 18000: 0.039038\n",
      "Loss after iteration 19000: 0.039037\n",
      "Loss after iteration 20000: 0.039037\n",
      "Loss after iteration 21000: 0.039036\n",
      "Loss after iteration 22000: 0.039036\n",
      "Loss after iteration 23000: 0.039036\n",
      "Loss after iteration 24000: 0.039036\n",
      "Loss after iteration 25000: 0.039036\n",
      "Loss after iteration 26000: 0.039036\n",
      "Loss after iteration 27000: 0.039036\n",
      "Loss after iteration 28000: 0.039036\n",
      "Loss after iteration 29000: 0.039036\n",
      "========================== /n\n",
      "print W1  [[ 5.79792691 -5.1662894   3.50820489  8.50286947  4.85545276]\n",
      " [-6.14572337 -4.78160272 -0.52529434  2.56708086  7.59408514]]\n",
      "print W2  [[-4.47346249  4.5090444 ]\n",
      " [ 4.66272693 -4.64307588]\n",
      " [-6.66708069  6.68439032]\n",
      " [ 6.47786573 -6.44917162]\n",
      " [ 4.28759994 -4.29964437]]\n",
      "print b1  [[ 3.25886473  0.35321137 -4.0656376  -3.9486942  -4.0195739 ]]\n",
      "print b2  [[ 0.44356906 -0.44356906]]\n",
      "print the loss 0.0390359328225\n"
     ]
    }
   ],
   "source": [
    "eps = .01\n",
    "reg = .01\n",
    "hid_n = 5\n",
    "inp_n = 2\n",
    "out_n = 2\n",
    "\n",
    "print \"the loss was .050 \"\n",
    "print \"          \"\n",
    "#build our model \n",
    "model = build_model(x ,y , hid_n , inp_n , out_n , reg  , eps , 30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's decrease the reg_lambda\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss was .0390359328225\n",
      "          \n",
      "Loss after iteration 0: 0.613664\n",
      "Loss after iteration 1000: 0.047437\n",
      "Loss after iteration 2000: 0.034416\n",
      "Loss after iteration 3000: 0.029516\n",
      "Loss after iteration 4000: 0.026723\n",
      "Loss after iteration 5000: 0.024821\n",
      "Loss after iteration 6000: 0.023405\n",
      "Loss after iteration 7000: 0.022285\n",
      "Loss after iteration 8000: 0.021368\n",
      "Loss after iteration 9000: 0.021595\n",
      "Loss after iteration 10000: 0.021657\n",
      "Loss after iteration 11000: 0.021705\n",
      "Loss after iteration 12000: 0.021729\n",
      "Loss after iteration 13000: 0.021722\n",
      "Loss after iteration 14000: 0.021662\n",
      "Loss after iteration 15000: 0.021532\n",
      "Loss after iteration 16000: 0.021338\n",
      "Loss after iteration 17000: 0.021120\n",
      "Loss after iteration 18000: 0.020918\n",
      "Loss after iteration 19000: 0.020750\n",
      "Loss after iteration 20000: 0.020615\n",
      "Loss after iteration 21000: 0.020504\n",
      "Loss after iteration 22000: 0.020412\n",
      "Loss after iteration 23000: 0.020334\n",
      "Loss after iteration 24000: 0.020267\n",
      "Loss after iteration 25000: 0.020209\n",
      "Loss after iteration 26000: 0.020158\n",
      "Loss after iteration 27000: 0.020114\n",
      "Loss after iteration 28000: 0.020075\n",
      "Loss after iteration 29000: 0.020040\n",
      "========================== /n\n",
      "print W1  [[  7.11897268 -10.96397842   3.51749209  12.86734522   7.39164831]\n",
      " [ -7.73241016 -10.43245872  -0.4869682    4.66325582  12.863097  ]]\n",
      "print W2  [[ -5.24941609   5.77894411]\n",
      " [  8.41117919  -8.11873339]\n",
      " [-11.52672529  11.78432619]\n",
      " [ 12.24314556 -11.8161216 ]\n",
      " [  7.66801536  -7.84725983]]\n",
      "print b1  [[ 3.92566241  1.6153532  -3.93436555 -6.08375439 -6.36478943]]\n",
      "print b2  [[-0.83560612  0.83560612]]\n",
      "print the loss 0.0187920845245\n"
     ]
    }
   ],
   "source": [
    "eps = .01\n",
    "reg = .001\n",
    "hid_n = 5\n",
    "inp_n = 2\n",
    "out_n = 2\n",
    "\n",
    "print \"the loss was .0390359328225\"\n",
    "print \"          \"\n",
    "#build our model \n",
    "model = build_model(x ,y , hid_n , inp_n , out_n , reg  , eps , 30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's decrease the reg_lambda again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss was .0187920845245\n",
      "          \n",
      "Loss after iteration 0: 0.613641\n",
      "Loss after iteration 1000: 0.046749\n",
      "Loss after iteration 2000: 0.033328\n",
      "Loss after iteration 3000: 0.028110\n",
      "Loss after iteration 4000: 0.025031\n",
      "Loss after iteration 5000: 0.022867\n",
      "Loss after iteration 6000: 0.021200\n",
      "Loss after iteration 7000: 0.019840\n",
      "Loss after iteration 8000: 0.018695\n",
      "Loss after iteration 9000: 0.018848\n",
      "Loss after iteration 10000: 0.018484\n",
      "Loss after iteration 11000: 0.018159\n",
      "Loss after iteration 12000: 0.017844\n",
      "Loss after iteration 13000: 0.017504\n",
      "Loss after iteration 14000: 0.017122\n",
      "Loss after iteration 15000: 0.016730\n",
      "Loss after iteration 16000: 0.016373\n",
      "Loss after iteration 17000: 0.016065\n",
      "Loss after iteration 18000: 0.015799\n",
      "Loss after iteration 19000: 0.015562\n",
      "Loss after iteration 20000: 0.015347\n",
      "Loss after iteration 21000: 0.015149\n",
      "Loss after iteration 22000: 0.014965\n",
      "Loss after iteration 23000: 0.014794\n",
      "Loss after iteration 24000: 0.014634\n",
      "Loss after iteration 25000: 0.014483\n",
      "Loss after iteration 26000: 0.014341\n",
      "Loss after iteration 27000: 0.014206\n",
      "Loss after iteration 28000: 0.014077\n",
      "Loss after iteration 29000: 0.013955\n",
      "========================== /n\n",
      "print W1  [[  7.59904624 -12.60837063   3.53679665  13.98087367   7.91417056]\n",
      " [ -8.27037501 -11.76108214  -0.46198259   5.20729574  13.9351186 ]]\n",
      "print W2  [[ -5.3697977    6.06346161]\n",
      " [  9.66154116  -9.27844698]\n",
      " [-12.9852593   13.32270782]\n",
      " [ 13.97785529 -13.41846826]\n",
      " [  8.83450054  -9.06930478]]\n",
      "print b1  [[ 4.14068608  1.93598769 -3.90634972 -6.661228   -6.81636369]]\n",
      "print b2  [[-1.06158414  1.06158414]]\n",
      "print the loss 0.0151979689841\n"
     ]
    }
   ],
   "source": [
    "eps = .01\n",
    "reg = .0001\n",
    "hid_n = 5\n",
    "inp_n = 2\n",
    "out_n = 2\n",
    "\n",
    "print \"the loss was .0187920845245\"\n",
    "print \"          \"\n",
    "#build our model \n",
    "model = build_model(x ,y , hid_n , inp_n , out_n , reg  , eps , 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally the loss become 0.0151979689841\n",
    "and it would be better if we increase the num of passes again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
